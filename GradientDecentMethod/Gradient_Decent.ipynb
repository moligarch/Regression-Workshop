{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Decent Method\n",
        "\n",
        "The provided code implements linear regression using gradient descent. Linear regression is a machine learning algorithm used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and aims to find the best-fitting line that minimizes the difference between the predicted and actual values.\n",
        "\n",
        "The code defines a `LinReg` class that encapsulates the functionality of linear regression."
      ],
      "metadata": {
        "id": "o5x2Y4lCVeW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinReg:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.m = X.shape[0]\n",
        "        self.n = X.shape[1]\n",
        "        self.w = np.zeros(self.n)\n",
        "        self.b = 0\n",
        "\n",
        "    def GetCostDerivation(self, spec):\n",
        "        #calculate f_{w,b}(X)\n",
        "        def f(self, index):\n",
        "            return np.dot(self.w, self.X[index]) + self.b\n",
        "\n",
        "        if spec == 'w': # Return dJ/dw\n",
        "            dJdW = np.zeros(self.n)\n",
        "            for dim in range (self.n):\n",
        "                for sample in range(self.m):\n",
        "                    dJdW[dim] += ((f(self,sample)-y[sample]) * X[sample][dim])/self.m\n",
        "            return  dJdW\n",
        "        elif spec == 'b': # Return dJ/db\n",
        "            dJdB = 0\n",
        "            for sample in range(self.m):\n",
        "                dJdB += (f(self, sample)-y[sample])/self.m\n",
        "            return dJdB\n",
        "\n",
        "    def GradDecent(self, lr):\n",
        "        self.w = self.w - lr * self.GetCostDerivation('w')\n",
        "        self.b = self.b - lr * self.GetCostDerivation('b')\n",
        "\n",
        "    def Compute(self, method, lr, n_iter):\n",
        "        for _ in range(n_iter):\n",
        "            method(lr)"
      ],
      "metadata": {
        "id": "NKV-ZzsqEJTD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Example\n",
        "\n",
        "In this example, a simple dataset is defined with input features `X` and target values `y`. An instance of the `LinReg` class is created with the dataset.\n",
        "\n",
        "The `Compute` method is called on the `linreg` instance with `linreg.GradDecent` as the optimization method, a learning rate of 0.05, and 1000 iterations. This performs gradient descent optimization to find the optimal weights and bias for linear regression.\n",
        "\n",
        "Finally, the final weights and bias are printed, representing the learned linear regression model.\n",
        "\n",
        "Output:\n",
        "```\n",
        "Weights: [1.07161243 0.25365249]\n",
        "Bias: -0.6801122411924745\n",
        "```\n",
        "\n",
        "The final weights are approximately `[1.07, 0.25]` and the bias is approximately `-0.68`, indicating that the learned linear regression model is `y = 1.07*x_1 + 0.25*x_2 - 0.68`.\n"
      ],
      "metadata": {
        "id": "H0gXgvgmVKhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple dataset\n",
        "X = np.array([[1,2], [2,4], [3,5], [4,3], [5,7]])\n",
        "y = np.array([2, 3, 1, 4, 8])\n",
        "\n",
        "# Create an instance of the LinReg class\n",
        "linreg = LinReg(X, y)\n",
        "\n",
        "# Perform gradient descent to find linear regression\n",
        "linreg.Compute(linreg.GradDecent, lr=0.05, n_iter=1000)\n",
        "\n",
        "# Print the final weights and bias\n",
        "print(\"Weights:\", linreg.w) #\n",
        "print(\"Bias:\", linreg.b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDuTYGp7U1Hm",
        "outputId": "2d6d681c-af08-4e59-9622-ff23632ce07c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [1.07161243 0.25365249]\n",
            "Bias: -0.6801122411924745\n"
          ]
        }
      ]
    }
  ]
}